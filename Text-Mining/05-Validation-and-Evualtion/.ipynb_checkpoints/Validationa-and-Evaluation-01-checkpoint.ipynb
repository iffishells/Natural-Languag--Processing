{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold Out Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(movies_review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewDocs_x = df.iloc[:,0]\n",
    "# reviewDocs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewDocs_y = df.iloc[:,1]\n",
    "# reviewDocs_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(reviewDocs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(tfidf,\n",
    "                                                reviewDocs_y,\n",
    "                                                train_size=0.1\n",
    "                                                 , \n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'negative', ..., 'negative', 'negative',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = clf.predict(test_x)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accc = accuracy_score(test_y,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8471333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \",accc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put into the Box   \n",
    "### ***Hold out Cross validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import train_test_split # bcz Hold out appraoch\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    " \n",
    "reviewDocs_x = df.iloc[:,0]  ## x component\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(reviewDocs_x)  # model language\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(tfidf,\n",
    "                                                reviewDocs_y,\n",
    "                                                train_size=0.8\n",
    "                                                 , \n",
    "                                                shuffle=True)  ## spliting data according to hold out cross validation\n",
    "\n",
    "clf = MultinomialNB().fit(train_x, train_y)  ## put into the Classifer\n",
    "\n",
    "pred_y = clf.predict(test_x)  # prediction\n",
    "# pred_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accc = accuracy_score(test_y,pred_y)\n",
    "print(\"Accuracy : \",accc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***k fold Cross Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        One of the other reviewers has mentioned that ...\n",
      "1        A wonderful little production. <br /><br />The...\n",
      "2        I thought this was a wonderful way to spend ti...\n",
      "3        Basically there's a family where a little boy ...\n",
      "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
      "                               ...                        \n",
      "49995    I thought this movie did a down right good job...\n",
      "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    I am a Catholic taught in parochial elementary...\n",
      "49998    I'm going to have to disagree with the previou...\n",
      "49999    No one expects the Star Trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n",
      "Accuracy :  [0.8585, 0.8643, 0.8583, 0.854, 0.8612]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    " \n",
    "x = df.iloc[:,0]  ## x component\n",
    "print(x)\n",
    "y = df.iloc[:,1]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(x)  # model language\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "acc = []\n",
    "kf  = KFold(n_splits=5)\n",
    "for trian_ids ,test_ids in kf.split(tfidf,y):\n",
    "    train_x , test_x = tfidf[trian_ids],tfidf[test_ids]\n",
    "    \n",
    "    train_y ,test_y = y[trian_ids] , y[test_ids]\n",
    "    \n",
    "    clf = MultinomialNB().fit(train_x, train_y)  ## put into the Classifer\n",
    "    pred_y = clf.predict(test_x)\n",
    "    acc.append(accuracy_score(test_y,pred_y))\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "print(\"Accuracy : \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Leave one Out Cross Validation***\n",
    "This is too much cost when the data is very large  \n",
    "k fold is very good when the data is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    "df =  df.head(500) \n",
    "x = df.iloc[:,0]  ## x component\n",
    "\n",
    "y = df.iloc[:,1]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(x)  # model language\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "acc = []\n",
    "loo = LeaveOneOut() \n",
    "for trian_ids ,test_ids in loo.split(x,y):\n",
    "    train_x , test_x = tfidf[trian_ids],tfidf[test_ids]\n",
    "    \n",
    "    train_y ,test_y = y[trian_ids] , y[test_ids]\n",
    "    \n",
    "    clf = MultinomialNB().fit(train_x, train_y)  ## put into the Classifer\n",
    "    pred_y = clf.predict(test_x)\n",
    "    acc.append(accuracy_score(test_y,pred_y))\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "print(\"Accuracy : \",sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Predictive Accuracy***\n",
    "formaula is following  \n",
    "Accuracy =  Number of correct classification / Number of instance classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier  #update\n",
    "from sklearn.model_selection import KFold\n",
    "import  pandas as pd\n",
    "# from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    "df =  df.head(500)  ## capture limited data\n",
    "x = df.iloc[:,0]  ## x component\n",
    "\n",
    "y = df.iloc[:,1]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=3000).fit_transform(x)  # model language\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)  ## put into the Classifer\n",
    "    \n",
    "# ----------------------------------------------------------------------------\n",
    "acc = []\n",
    "kf = KFold(n_splits=5) \n",
    "score = 0 \n",
    "for trian_ids ,test_ids in kf.split(tfidf):\n",
    "    train_x , test_x = tfidf[trian_ids],tfidf[test_ids]\n",
    "    \n",
    "    train_y ,test_y = y[trian_ids] , y[test_ids]\n",
    "    \n",
    "    knn.fit(train_x, train_y)  ## put into the Classifer\n",
    "    pred_y = knn.predict(test_x)\n",
    "#     acc.append(accuracy_score(test_y,pred_y))\n",
    "    score += accuracy_score(test_y,pred_y,normalize=True) \n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "print(score/5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>  ***Precision , Recall , and f1 Score*** </center> \n",
    "These are techniques to Evauation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy Score 0.8545 \n",
      " Precision Score 0.8545192911627728 \n",
      " F1 Score 0.8544923383762003 \n",
      " Recall Score 0.8545 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import train_test_split # bcz Hold out appraoch\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    " \n",
    "reviewDocs_x = df.iloc[:,0]  ## x component\n",
    "\n",
    "reviewDocs_y = df.iloc[:,1]  ## y component\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(reviewDocs_x)  # model language\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(tfidf,\n",
    "                                                reviewDocs_y,\n",
    "                                                train_size=0.8\n",
    "                                                 , \n",
    "                                                shuffle=True)  ## spliting data according to hold out cross validation\n",
    "\n",
    "clf = MultinomialNB().fit(train_x, train_y)  ## put into the Classifer\n",
    "\n",
    "pred_y = clf.predict(test_x)  # prediction\n",
    "# pred_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score , precision_score,f1_score,recall_score\n",
    "accc = accuracy_score(test_y,pred_y)\n",
    "pre_score = precision_score(test_y,pred_y , average=\"weighted\")\n",
    "f1Score = f1_score(test_y,pred_y, average=\"weighted\")\n",
    "recallScore = recall_score(test_y,pred_y, average=\"weighted\")\n",
    "\n",
    "print(\" Accuracy Score {} \\n Precision Score {} \\n F1 Score {} \\n Recall Score {} \".format(accc,pre_score,f1Score,recallScore))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Confusion matrix***  \n",
    "Thus in binary classification, the count of true negatives is ConfusionMatrix[0,0]  \n",
    "false negatives is   C[1,0],\n",
    "true positives is C[1,1]   and \n",
    "false positives is C[0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Confusion Matrix \n",
      " [[4348  659]\n",
      " [ 756 4237]]   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import train_test_split # bcz Hold out appraoch\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    " \n",
    "reviewDocs_x = df.iloc[:,0]  ## x component\n",
    "\n",
    "reviewDocs_y = df.iloc[:,1]  ## x component\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(reviewDocs_x)  # model language\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(tfidf,\n",
    "                                                reviewDocs_y,\n",
    "                                                train_size=0.8\n",
    "                                                 , \n",
    "                                                shuffle=True)  ## spliting data according to hold out cross validation\n",
    "\n",
    "clf = MultinomialNB().fit(train_x, train_y)  ## put into the Classifer\n",
    "\n",
    "pred_y = clf.predict(test_x)  # prediction\n",
    "# pred_y\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CFM = confusion_matrix(test_y,pred_y)\n",
    "\n",
    "print(\" Confusion Matrix \\n {}   \".format(CFM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***All puts together***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score from MultinomiaNB 0.8562666666666667 \n",
      " F1 Score from Decision Tree 0.6733333333333333 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer ## Vectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB  ## classifer\n",
    "from sklearn.tree import DecisionTreeClassifier ## classifer\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import KFold    ## cross validation\n",
    "from sklearn.metrics import f1_score    ## evluation\n",
    "\n",
    "\n",
    "movies_review_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/IMDB Dataset.csv\"\n",
    "\n",
    "df  = pd.read_csv(movies_review_path) # reading file\n",
    "df = df.head(15000) \n",
    "x = df.iloc[:,0]  ## x component\n",
    "\n",
    "y = df.iloc[:,1]  \n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=20,\n",
    "                        max_df=300000).fit_transform(x)  # model language\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# marjor task\n",
    "\n",
    "Mnb = MultinomialNB()\n",
    "dt = DecisionTreeClassifier(random_state=0,max_depth=3)\n",
    "kf  = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "## Count \n",
    "Mnb_count = 0\n",
    "dt_count = 0\n",
    "for trian_ids ,test_ids in kf.split(tfidf,y): # y optional bcz already defined above\n",
    "    train_x , test_x = tfidf[trian_ids],tfidf[test_ids]\n",
    "    \n",
    "    train_y ,test_y = y[trian_ids] , y[test_ids]\n",
    "    \n",
    "    Mnb.fit(train_x, train_y)  ## put into the Classifer\n",
    "    Mnb_pred_y = Mnb.predict(test_x)\n",
    "    Mnb_count += f1_score(test_y,Mnb_pred_y , average=\"micro\")\n",
    "    \n",
    "    ## Decision Tree\n",
    "    dt.fit(train_x, train_y)\n",
    "    dt_pred_y = dt.predict(test_x)\n",
    "    dt_count += f1_score(test_y,dt_pred_y , average = \"micro\")\n",
    "\n",
    "    \n",
    "Mnb_count=Mnb_count/5\n",
    "dt_count = dt_count/5\n",
    "\n",
    "print(\"F1 Score from MultinomiaNB {} \\n F1 Score from Decision Tree {} \".format(Mnb_count,dt_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Clustering Evaluation*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 21: expected 8 fields, saw 17\\nSkipping line 28: expected 8 fields, saw 10\\nSkipping line 34: expected 8 fields, saw 13\\nSkipping line 37: expected 8 fields, saw 9\\nSkipping line 52: expected 8 fields, saw 11\\nSkipping line 140: expected 8 fields, saw 9\\nSkipping line 245: expected 8 fields, saw 9\\nSkipping line 296: expected 8 fields, saw 10\\nSkipping line 316: expected 8 fields, saw 11\\n'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-07b74d4af088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmoview_review_data_text_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/dataset-CalheirosMoroRita-2017.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoview_review_data_text_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unicode_escape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "moview_review_data_text_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/dataset-CalheirosMoroRita-2017.csv\"\n",
    "\n",
    "df = pd.read_csv(moview_review_data_text_path,error_bad_lines=False ,encoding=\"unicode_escape\").read()\n",
    "\n",
    "df = df.T\n",
    "df\n",
    "\n",
    "## Underprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>***Assignment Model Validation Evalution*** </center> \n",
    "Assignment: Assignment 4: Model Evaluation  \n",
    "60 minutes \n",
    "training multiple classifiers on the same data and evaluating them to analyze which one is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score from MultinomiaNB 0.55275 \n",
      "F1 Score from Decision Tree (depth =3)0.1423450021324904 \n",
      "F1 Score from Decision Tree(depth=30) 0.55275  \n",
      "F1 Score from K Neighbour Model(N=3) 0.55275 \n",
      "F1 Score from K Neighbour Model(N=5) 0.1423450021324904\n"
     ]
    }
   ],
   "source": [
    "## importing all need of lib\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import  pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "## load file  \n",
    "Assignment_text_path = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/__sentiDataset.txt\"\n",
    "from sklearn.metrics import accuracy_score , precision_score,f1_score,recall_score\n",
    "\n",
    "df = open(Assignment_text_path).read()\n",
    "\n",
    "df = df.split(\"\\n\")\n",
    "# df = df[:30]\n",
    "\n",
    "## lenght each document  \n",
    "splitted_by_line  = df[0].split(\"\\t\")\n",
    "\n",
    "df_dict = {\n",
    "    \"input\" : [],\n",
    "    \"label\":[]\n",
    "    \n",
    "}\n",
    "\n",
    "for doc_index in range(0,len(df)):\n",
    "    single_doc = df[doc_index].split(\"\\t\")\n",
    "    df_dict[\"label\"].append(single_doc[2])\n",
    "    df_dict[\"input\"].append(str(single_doc[3]))\n",
    "\n",
    "## load into the DataFram \n",
    "data  = pd.DataFrame(df_dict)\n",
    "\n",
    "\n",
    "\n",
    "## pre-processing of data\n",
    "\n",
    "\n",
    "def remove_string_special_characters(s):\n",
    "    s = str(s)  \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "#     stripped = re.sub('.', '', stripped)\n",
    "       \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "#     print(row[\"input\"])\n",
    "    doc = remove_string_special_characters(row[\"input\"]) \n",
    "#     print(doc)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "  \n",
    "    word_tokens = word_tokenize(example_sent)  \n",
    "    \n",
    "    filtered_sentence = []  \n",
    "\n",
    "    for w in word_tokens:  \n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(porter_stemmer.stem(w))  ## stemming\n",
    "#     print(word_tokens)  \n",
    "    row[\"input\"] = str(filtered_sentence)\n",
    "    \n",
    "    \n",
    "    \n",
    "x = data.iloc[:,0]  ## x component\n",
    "# print(x)\n",
    "y = data.iloc[:,1]\n",
    "\n",
    "# print(y)\n",
    "tfidf = TfidfVectorizer(max_features=200 ,ngram_range= (2,2)).fit_transform(x)  # model language\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "Mnb = MultinomialNB()\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0,max_depth=3 )\n",
    "dt_2 = DecisionTreeClassifier(random_state=0,max_depth=30)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh_2 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "kf  = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "## Count \n",
    "Mnb_count = 0\n",
    "dt_count = 0\n",
    "dt_2_count =0\n",
    "neigh_count=0\n",
    "neigh_2_count=0\n",
    "\n",
    "for trian_ids ,test_ids in kf.split(tfidf,y): # y optional bcz already defined above\n",
    "    train_x , test_x = tfidf[trian_ids],tfidf[test_ids]\n",
    "    \n",
    "    train_y ,test_y = y[trian_ids] , y[test_ids]\n",
    "    \n",
    "    Mnb.fit(train_x, train_y)  ## put into the Classifer\n",
    "    Mnb_pred_y = Mnb.predict(test_x)\n",
    "    Mnb_count += f1_score(test_y,Mnb_pred_y , average=\"micro\")\n",
    "    \n",
    "    ## Decision Tree\n",
    "    dt.fit(train_x, train_y)\n",
    "    dt_pred_y = dt.predict(test_x)\n",
    "    dt_count += f1_score(test_y,dt_pred_y , average = \"macro\")\n",
    "    # model 2\n",
    "    dt_2.fit(train_x, train_y)\n",
    "    dt_2_pred_y = dt.predict(test_x)\n",
    "    dt_2_count += f1_score(test_y,dt_pred_y , average = \"micro\")\n",
    "    \n",
    "   \n",
    "    ## KNN Neighbour\n",
    "    neigh.fit(train_x, train_y)\n",
    "    neigh_pred_y = dt.predict(test_x)\n",
    "    neigh_count += f1_score(test_y,dt_pred_y , average = \"micro\")\n",
    "    \n",
    "    neigh_2.fit(train_x, train_y)\n",
    "    neigh_2_pred_y = dt.predict(test_x)\n",
    "    neigh_2_count += f1_score(test_y,dt_pred_y , average = \"macro\")\n",
    "    \n",
    "    \n",
    "    \n",
    "Mnb_count=Mnb_count/5\n",
    "dt_count = dt_count/5\n",
    "neigh_count = neigh_count/5\n",
    "neigh_2_count = neigh_2_count/5\n",
    "dt_2_count = dt_2_count/5\n",
    "\n",
    "print(\"F1 Score from MultinomiaNB {} \\nF1 Score from Decision Tree (depth =3){} \\nF1 Score from Decision Tree(depth=30) {}  \\nF1 Score from K Neighbour Model(N=3) {} \\nF1 Score from K Neighbour Model(N=5) {}\" .format(Mnb_count,dt_count,dt_2_count,neigh_count,neigh_2_count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Pre-Processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment : \n",
    "Read the dataset\n",
    "\n",
    "import the required resources\n",
    "\n",
    "Preprocess to transform the dataset into preprocessed form.\n",
    "\n",
    "Questions for this assignment\n",
    "Perform the following,\n",
    "\n",
    "Preprocess the given dataset by,\n",
    "\n",
    "i) removing stopwords using ENGLISH_STOP_WORDS\n",
    "\n",
    "ii) punctuation\n",
    "\n",
    "iii) any words whose POS is not NN or Verb\n",
    "\n",
    "\n",
    "\n",
    "Hint: there are many types of verb so either look for each type of just keep a check if the first two characters of POS are VB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "#reading file\n",
    "\n",
    "EngStopWord = open(\"EnglishStopWords.txt\").read()\n",
    "EngStopWord= EngStopWord.split(\"\\n\")\n",
    "\n",
    "filePathForPreProcessing = \"/home/iffishells/Desktop/Data-Science/Text-Mining/Datasets/__sentiDataset.txt\"\n",
    "df = open(filePathForPreProcessing).read()\n",
    "\n",
    "\n",
    "## removing Stop words from the file\n",
    "# df = df[:1000]  ## tool small section\n",
    "\n",
    "\n",
    "\n",
    "## converting into the lower Case:\n",
    "df = df.lower()\n",
    "\n",
    "## reomving white space \n",
    "df = df.strip()\n",
    "\n",
    "## Removing punctuation\n",
    "from string import punctuation as punc\n",
    "\n",
    "for ch in df:\n",
    "    if ch in punc:\n",
    "        df.replace(ch,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.split(\" \")\n",
    "\n",
    "print(\" Before removing StopWord Len : \",len(df))\n",
    "\n",
    "for word in df:\n",
    "#     print(word)\n",
    "    if word in EngStopWord:\n",
    "        df.remove(word)\n",
    "print(\" After removing StopWord Len : \",len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtien Distance Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pycountry \n",
    "import tkinter as tk  \n",
    "from functools import partial  \n",
    "   \n",
    "def levenshtein(seq1 , seq2):\n",
    "    \n",
    "    size_x = len(seq1)+1\n",
    "#     print(\"len seq1 : \",size_x)\n",
    "    size_y = len(seq2)+1\n",
    "#     print(\"len seq2 : \",size_y )\n",
    "    matrix = np.zeros((size_x,size_y))\n",
    "    \n",
    "    ## inialization of row 0 to len(size_x)\n",
    "    for x in range(size_x):\n",
    "        matrix[x,0] = x\n",
    "        \n",
    "#     print(\"X \\n\" ,matrix)\n",
    "    \n",
    "    ## inialization of columns 0 to len(Seq2)\n",
    "    for y in range(size_y):\n",
    "        matrix[0,y] = y\n",
    "        \n",
    "#     print(\"Y \\n\",matrix)\n",
    "    \n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix[x,y] = min(matrix[x-1,y],matrix[x-1,y-1],matrix[x,y-1])\n",
    "#                 print(\"X loop = \".join(x) , matrix)\n",
    "            else:\n",
    "                matrix[x,y] = min(\n",
    "                matrix[x-1,y]+1,\n",
    "                matrix[x-1,y-1]+1,\n",
    "                matrix[x,y-1]+1\n",
    "                )\n",
    "#                 print(\"y = \",x ,\"\\n\", matrix)\n",
    "                \n",
    "#     print(\"final : \\n\", matrix)\n",
    "    \n",
    "    return (matrix[size_x-1,size_y-1])\n",
    "        \n",
    "    \n",
    "def Sort_Tuple(tup):  \n",
    "      \n",
    "    # getting length of list of tuples \n",
    "    lst = len(tup)  \n",
    "    for i in range(0, lst):  \n",
    "          \n",
    "        for j in range(0, lst-i-1):  \n",
    "            if (tup[j][1] > tup[j + 1][1]):  \n",
    "                temp = tup[j]  \n",
    "                tup[j]= tup[j + 1]  \n",
    "                tup[j + 1]= temp  \n",
    "    return tup \n",
    "\n",
    "def function_Calls(label_result,target):\n",
    "    target = str(target.get())\n",
    "    \n",
    "    ##import country from pycountry Python API\n",
    "    country_name=[]\n",
    "    for name in range(20):\n",
    "        country = list(pycountry.countries)[name].name\n",
    "        country_name.append(country)\n",
    "\n",
    "    # compare with the target from the list of the name (Name can be random) \n",
    "    ## but am added all the country name\n",
    "    \n",
    "    edit_distance = []\n",
    "    for name in country_name:\n",
    "    #     distace = levenshtein(\"Apple\" , name)\n",
    "        edit_distance.append((name,int(levenshtein(target , name))))\n",
    "      \n",
    "    result = str(Sort_Tuple(edit_distance)[0:3])\n",
    "    label_result.config(text=\"Result = %s\" % result)\n",
    "    return\n",
    "#     Sort_Tuple(edit_distance)[0:3]\n",
    "\n",
    "\n",
    "root = tk.Tk()  \n",
    "root.geometry('800x500')\n",
    "root.resizable(0,0)\n",
    "  \n",
    "root.title('Levenshtein Distance')  \n",
    "#    \n",
    "number1 = tk.StringVar()    \n",
    "  \n",
    "labelNum1 = tk.Label(root, text=\"Target String\").grid(row=1, column=0)  \n",
    "   \n",
    "labelResult = tk.Label(root) ## WILL HOLD THE RESULT OF OUTPUT  \n",
    "  \n",
    "labelResult.grid(row=9, column=1)  \n",
    "  \n",
    "entryNum1 = tk.Entry(root, textvariable=number1).grid(row=1, column=2)  \n",
    "  \n",
    "\n",
    "function_Calls = partial(function_Calls, labelResult, number1)  \n",
    "  \n",
    "buttonCal = tk.Button(root, text=\"Submit\", command=function_Calls).grid(row=3, column=0)  \n",
    "  \n",
    "root.mainloop()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Using cached pycountry-20.7.3-py2.py3-none-any.whl\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-20.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1d2d21542286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import the enchant module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# determining the values of the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstring1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello World\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "# import the enchant module\n",
    "import enchant\n",
    "\n",
    "# determining the values of the parameters\n",
    "string1 = \"Hello World\"\n",
    "string2 = \"Hello d\"\n",
    "\n",
    "# the Levenshtein distance between\n",
    "# string1 and string2\n",
    "print(enchant.utils.levenshtein(string1, string2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement enchant\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for enchant\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install enchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4.]\n",
      " [1. 0. 1. 2. 3.]\n",
      " [2. 1. 0. 1. 2.]\n",
      " [3. 2. 1. 1. 2.]\n",
      " [4. 3. 2. 1. 2.]\n",
      " [5. 4. 3. 2. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(\"Apple\",\"Aple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<', 's'), ('s', '>'), ('>', 'I'), ('I', 'am'), ('am', 'Sam'), ('Sam', '<'), ('<', '/s'), ('/s', '>'), ('>', '<'), ('<', 's'), ('s', '>'), ('>', 'Sam'), ('Sam', 'I'), ('I', 'am'), ('am', '<'), ('<', '/s'), ('/s', '>'), ('>', '<'), ('<', 's'), ('s', '>'), ('>', 'I'), ('I', 'and'), ('and', 'Sam'), ('Sam', '<'), ('<', '/s'), ('/s', '>'), ('>', '<'), ('<', 's'), ('s', '>'), ('>', 'I'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'beef'), ('beef', '<'), ('<', '/s'), ('/s', '>')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_data = \"<s> I am Sam </s> <s> Sam I am </s> <s> I and Sam </s> <s> I do not like green eggs and beef </s>\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "\n",
    "lip = list(nltk.bigrams(nltk_tokens))\n",
    "print(lip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-30-add1c9826423>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-add1c9826423>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    if flag == False:\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# Python3 code to convert tuple\n",
    "# into string\n",
    "def count(listOfTuple):\n",
    "    flag = False\n",
    "\n",
    "    # To append Duplicate elements in list\n",
    "    coll_list = []\n",
    "    coll_cnt = 0\n",
    "    for t in listOfTuple:\n",
    "        # To check if Duplicate exist\n",
    "        if t in coll_list:\n",
    "            flag = True\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            coll_cnt = 0\n",
    "            for b in listOfTuple:\n",
    "                if b[0] == t[0] and b[1] == t[1]:\n",
    "                    coll_cnt = coll_cnt + 1\n",
    "\n",
    "            # To print count if Duplicate of element exist\n",
    "            if(coll_cnt > 1):\n",
    "                print(t, \"-\", coll_cnt)\n",
    "            coll_list.append(t)\n",
    "\n",
    "\tif flag == False:\n",
    "\t\tprint(\"No Duplicates\")\n",
    "\n",
    "# Driver code\n",
    "print(\"Test Case 1:\")\n",
    "listOfTuple = [('a', 'e'), ('b', 'x'), ('b', 'x'),\n",
    "\t\t\t('a', 'e'), ('b', 'x')]\n",
    "\n",
    "count(listOfTuple)\n",
    "\n",
    "print(\"Test Case 2:\")\n",
    "listOfTuple = [(0, 5), (6, 9), (0, 8)]\n",
    "count(listOfTuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "# raw_html = raw_html.read()\n",
    "\n",
    "# article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "article_paragraphs = \"<s> I am Sam </s> <s> Sam I am </s> <s> I and Sam </s> <s> I do not like green eggs and beef </s>\"\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text\n",
    "\n",
    "corpus = nltk.sent_tokenize(article_text)\n",
    "\n",
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
    "\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "\n",
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in corpus:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "    \n",
    "    \n",
    "word_tf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in corpus:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
