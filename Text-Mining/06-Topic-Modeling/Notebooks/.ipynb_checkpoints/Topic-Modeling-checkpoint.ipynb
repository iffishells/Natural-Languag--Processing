{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b081499",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "### 01 Default Topic Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f4da46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compponents : \n",
      " [[ 0.33371676  0.33371671 10.33313136]\n",
      " [ 0.36228994 20.30299748  0.33343434]\n",
      " [20.3039933   0.3632858   0.3334343 ]]\n",
      "topic ID : 0\n",
      "words IDS :  [2 0 1]\n",
      "word :  ['pet', 'bread', 'milk']\n",
      "topic ID : 1\n",
      "words IDS :  [1 0 2]\n",
      "word :  ['milk', 'bread', 'pet']\n",
      "topic ID : 2\n",
      "words IDS :  [0 1 2]\n",
      "word :  ['bread', 'milk', 'pet']\n"
     ]
    }
   ],
   "source": [
    "corpus = ['bread bread bread bread bread bread bread bread bread bread',\n",
    "         'milk milk milk milk milk milk milk milk milk milk',\n",
    "         'pet pet pet pet pet pet pet pet pet pet',\n",
    "         'bread bread bread bread bread bread bread bread bread bread milk milk milk milk milk milk milk milk milk milk']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "vec = CountVectorizer(lowercase=True)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "matrixX = vec.fit_transform(corpus)  ## counter Vector izer\n",
    "\n",
    "features = vec.get_feature_names()\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3)\n",
    "lda.fit(matrixX)\n",
    "\n",
    "print(\"Compponents : \\n\",lda.components_)\n",
    "\n",
    "for tid,topic in enumerate(lda.components_):\n",
    "    print(\"topic ID :\",tid)\n",
    "    print(\"words IDS : \",topic.argsort()[::-1])\n",
    "    print(\"word : \",[features[i] for i in topic.argsort()[::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae1986",
   "metadata": {},
   "source": [
    "### 02 Topic Modeling using UCI Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97c9f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components : \n",
      " [[0.72276081 0.20444066 0.20002678 ... 0.20006314 0.20005232 0.20003568]\n",
      " [0.20045891 0.85039319 0.51969443 ... 0.21255988 0.30470302 0.62261786]\n",
      " [0.20001369 0.20011697 0.20003001 ... 0.4703575  0.2000589  0.20004064]\n",
      " [0.20001054 0.4619186  0.20002114 ... 0.20005534 0.20004294 0.20002802]\n",
      " [0.20000823 0.76662028 0.20001733 ... 0.3795579  0.48602655 0.20002212]]\n",
      "Topic :  0\n",
      "WordID  [2546 2306  540 ...  559 1106 1234]\n",
      "word : ['you', 'thank', 'back', 'love', 'to', 'come', 'place', 'beautiful', 'will']\n",
      "Topic :  1\n",
      "WordID  [2310  470 2472 ... 1758 1106  517]\n",
      "word : ['the', 'and', 'we', 'to', 'you', 'is', 'was', 'of', 'hotel']\n",
      "Topic :  2\n",
      "WordID  [2546 2306 1255 ... 1106    0 1234]\n",
      "word : ['you', 'thank', 'great', 'love', '155', '279', '163', '19', '238']\n",
      "Topic :  3\n",
      "WordID  [2310 2472 2308 ... 2290  517    0]\n",
      "word : ['the', 'we', 'thanks', 'and', 'to', 'so', 'of', 'this', 'in']\n",
      "Topic :  4\n",
      "WordID  [2310  470 1174 ...  559 1106    0]\n",
      "word : ['the', 'and', 'for', 'we', 'to', 'you', 'is', 'in', 'of']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "df = open(\"Datasets/Datasets.csv\").read()\n",
    "docs = df.split(\"\\n\")\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "matrixX =tfidf.fit_transform(docs)\n",
    " \n",
    "features = tfidf.get_feature_names()\n",
    "    \n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(matrixX)\n",
    "    \n",
    "    \n",
    "print(\"Components : \\n\",lda.components_)\n",
    "\n",
    "for tid,topic in enumerate(lda.components_):\n",
    "    print(\"Topic : \",tid)\n",
    "    print(\"WordID \",topic.argsort()[::-1])\n",
    "    print(\"word :\",[features[i] for i in topic.argsort()[:-10:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d37e1",
   "metadata": {},
   "source": [
    "### 03 LDA with HyperParameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3795e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic :  0\n",
      "WordID  [0 1 2]\n",
      "word : ['bread', 'milk', 'pet']\n",
      "Topic :  1\n",
      "WordID  [2 1 0]\n",
      "word : ['pet', 'milk', 'bread']\n",
      "Topic :  2\n",
      "WordID  [1 0 2]\n",
      "word : ['milk', 'bread', 'pet']\n"
     ]
    }
   ],
   "source": [
    "corpus = ['bread bread bread bread bread bread bread bread bread bread',\n",
    "         'milk milk milk milk milk milk milk milk milk milk',\n",
    "         'pet pet pet pet pet pet pet pet pet pet',\n",
    "         'bread bread bread bread bread bread bread bread bread bread milk milk milk milk milk milk milk milk milk milk'] \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec  = CountVectorizer()\n",
    "\n",
    "matrix_X = vec.fit_transform(corpus)\n",
    "features= vec.get_feature_names()\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=3 , topic_word_prior=0.1 , doc_topic_prior=0.1)\n",
    "lda.fit(matrixX)\n",
    "\n",
    "\n",
    "for tid,topic in enumerate(lda.components_):\n",
    "    print(\"Topic : \",tid)\n",
    "    print(\"WordID \",topic.argsort()[::-1])\n",
    "    print(\"word :\",[features[i] for i in topic.argsort()[:-10:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b38e90",
   "metadata": {},
   "source": [
    "### 04 Online LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285e7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components : \n",
      " [[0.9254465  1.72779267 0.75691672 ... 0.87805524 0.81466339 0.83221083]\n",
      " [0.52098404 0.51792382 0.51025146 ... 0.51268105 0.51419621 0.52846798]]\n",
      "Topic :  0\n",
      "WordID  [2310  470 2472 ...   33   62 1438]\n",
      "word : ['the', 'and', 'we', 'to', 'you', 'for', 'of', 'place', 'is']\n",
      "Topic :  1\n",
      "WordID  [ 331  564  873 ...  992 1567 2169]\n",
      "word : ['394', 'beautiful', 'dear', 'maria', 'joana', 'hello', 'dazzled', 'review', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "df = open(\"Datasets/Datasets.csv\").read()\n",
    "docs = df.split(\"\\n\")\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "matrixX =tfidf.fit_transform(docs)\n",
    " \n",
    "features = tfidf.get_feature_names()\n",
    "    \n",
    "lda = LatentDirichletAllocation(n_components=2 , max_iter=200,learning_offset=4.0,learning_method='online')\n",
    "lda.fit(matrixX)\n",
    "    \n",
    "    \n",
    "print(\"Components : \\n\",lda.components_)\n",
    "\n",
    "for tid,topic in enumerate(lda.components_):\n",
    "    print(\"Topic : \",tid)\n",
    "    print(\"WordID \",topic.argsort()[::-1])\n",
    "    print(\"word :\",[features[i] for i in topic.argsort()[:-10:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67aac3",
   "metadata": {},
   "source": [
    "### 05  perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47dabb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda1:  2644.8666986269363\n",
      "lda2:  2094.9233425456696\n"
     ]
    }
   ],
   "source": [
    "corpus = open(\"Datasets/Datasets.csv\").read()\n",
    "docs = corpus.split('\\n')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "matrix_X = vec.fit_transform(docs)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda1 = LatentDirichletAllocation(n_components = 3)\n",
    "lda2 = LatentDirichletAllocation(n_components = 2)\n",
    "\n",
    "lda1.fit(matrix_X[:500])\n",
    "lda2.fit(matrix_X[:500])\n",
    "\n",
    "print('lda1: ', lda1.perplexity(matrix_X[500:]))\n",
    "print('lda2: ', lda2.perplexity(matrix_X[500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307b017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
